{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        35\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  False\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxAAAACWCAYAAABO+G6lAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAI4UlEQVR4nO3ce6xlZ1nH8d9TOpTGIrZgKVESrYLQSqTxwqVIqtKIGEvCLRKVKJhAhKoBQ0BJrIISSE2EFIwKjKImagg0EAcaSi92aoeS2oAoaWgE/qAdRi7houPYlsc/9pp4Mjkz87Rzhn3G8/kkk7PXOmve/e6T94/9Pe/ap7o7AAAAE6etewIAAMCpQ0AAAABjAgIAABgTEAAAwJiAAAAAxgQEAAAwtvaAqKrvqaprjzh35wMYZ09VXbQ8flZVfaWqajl+c1X90mCM11fV5zbOp6ouqqqbq+ofq+q6qjp/OX/+cu6Gqrq+qr77GON+X1XdVlXfqKqnbTj/x1W1b/n3mg3nX1tVH6uqW6vqlff3ZwEAACfL2gNiC+1NcvHy+OIktyW5cMPxTYMx3p7kJ444d3eSZ3b305NcmeT3lvO/luSd3X1Jkr9Mcvkxxr07yaVJ3nPE+bd195OTPDXJs5fQeGiSFyc5fP5lVfVtg7mzA1XVg9Y9BwBgZzllAqKq3l5VL6qq06rqmqp60hGX7E1y+Lf7P5TkT5I8rarOSPLI7v7s8Z6ju+9O8s0jzu3v7q8vh4eS3Ls8/tck37E8PjvJgao6o6r2VtXjquq8ZQfh7O7+r+7+8ibP9+nl6zeXce9LcjDJXUnOXP4dTHLP8ebO9lRVF1bVLcsu1Qer6oJlXfxDVf19VV2xXHfnhv/zjqq6ZHl8zbLLdWtVPWU5d0VV/UVVvT/JC6rq8qq6aXmeX/3Wv0oAYCc5fd0TWPxwVd1wnGtemeS6rHYTPtLdHz3i+7cmeVdV7UrSWe04XJnkk0k+liTLG7A3bjL273f3dcd68mUX4A1JXrKcujbJNVX1kiRnJPmx7j60HO9O8tUkv9ndXznO60pV/UKSfz8cOVW1J8kdWQXeG7r7f443BtvWTyfZ3d1/VlWnJXlfkt/o7luq6s8H//853f2fVfX4JG9L8pPL+UPdfdly/sokT89qvdxUVe/r7i+dhNcCALBtAuK27n7G4YPNPgPR3f9dVbuTvDnJo47y/QNJnpPk9u4+UFXnZbUrsXe55pYkl9zfyS1R8ndJ3tTd/7acflOS13X3e6vqhUn+MMnLu/uOqvpMknO6+58GYz8jya8k+bnl+LFJnpvk/KzeEN5YVVd39+fv77zZFnYn+Z2q+pskn0jymKxiN0k+mmSzz84c/uzOmUneUlU/kNXu1HdtuObw2vrBJBckuX45/vYkj04iIDghVfWKJM9Lcmd329liLaxD1s0a3Nx2CYjjqqpHZfXb/9dn9WZ9sw8X703y6iS/vRzfleT5Wb1Bf0A7EMtvjf86ydXdffXGbyX54vL4QJJzlusvTbIryRer6rLufv8xXtOTltfzM919cMO4X+/uQ8s1h5KcdbQx2PYOdfdvJcny4fwvJPmRrOLhR7P6fEySfHUJ3v9I8sQkf5XkmUnu6+4fr6oLkmxcS/ctXz+V5PYkz+3urqpd3e2WN05Yd1+V5Kp1z4OdzTpk3azBzZ0SAbG8id+d1S1B+6rqb6vqWd2954hL9yZ5VZJ9y/HNSZ6d1W1Mx92BWCrz55M8fnmz99IkFyX52SSPrKpfTPIv3X15Vrcz/WlV3ZtVMLy0qs5N8gdZ3bZyb5Jrq+qfk3wtyXuz+k3xhVW1p7t/N8k7l6e+evmDUa/q7tuW+933ZRUT13f3HQ/gx8b28MKq+uWsbqvbn9W6eUdVfSn/F6DJamftw1l9tubAcu6WJK9d1uLNmw3e3Z9cvn9jVd2X5OASrvdudj0AwImq7l73HGBHWoL0+7v7inXPBQBg6pT5K0wAAMD62YEAAADG7EAAAABjAgIAABg75l9h+vXvfbf7m3aQt37mRbXuOWzmzIteYR3uIAdvv2rbrUNrcGfZjmswsQ53GuuQ7eBo69AOBAAAMCYgAACAMQEBAACMCQgAAGBMQAAAAGMCAgAAGBMQAADAmIAAAADGBAQAADAmIAAAgDEBAQAAjAkIAABgTEAAAABjAgIAABgTEAAAwJiAAAAAxgQEAAAwJiAAAIAxAQEAAIwJCAAAYExAAAAAYwICAAAYExAAAMCYgAAAAMYEBAAAMCYgAACAMQEBAACMCQgAAGBMQAAAAGMCAgAAGBMQAADAmIAAAADGBAQAADAmIAAAgDEBAQAAjAkIAABgTEAAAABjAgIAABgTEAAAwJiAAAAAxgQEAAAwJiAAAIAxAQEAAIwJCAAAYExAAAAAYwICAAAYExAAAMCYgAAAAMYEBAAAMCYgAACAMQEBAACMCQgAAGDs9K0Y5Nzv/HKe+uSPb8VQJ8W+W5+Q/V94xLqnwUn2iIsvzZ7X/NS6p3FUl/3Rjdl/wwfXPQ0AgBOyJQGxa9c9efg5X9uKoU6KBz/4nnVPgW+Bsx76kDzmvLPWPY2jetjDHpL9654EAMAJcgsTAAAwJiAAAIAxAQEAAIwJCAAAYExAAAAAYwICAAAYExAAAMCYgAAAAMYEBAAAMCYgAACAMQEBAACMCQgAAGBMQAAAAGMCAgAAGBMQAADAmIAAAADGBAQAADAmIAAAgDEBAQAAjAkIAABgTEAAAABjAgIAABgTEAAAwJiAAAAAxgQEAAAwJiAAAIAxAQEAAIwJCAAAYExAAAAAYwICAAAYExAAAMCYgAAAAMYEBAAAMCYgAACAMQEBAACMCQgAAGBMQAAAAGOnb9VA3Vs1EgAAsF1tSUB8/q5z8653X7YVQ8ED9tkPfSBnf+gD654GAMD/a1u0A1FbMwwAALCt+QwEAAAwJiAAAIAxAQEAAIwJCAAAYExAAAAAYwICAAAYExAAAMCYgAAAAMYEBAAAMCYgAACAMQEBAACMCQgAAGBMQAAAAGMCAgAAGBMQAADAmIAAAADGBAQAADAmIAAAgDEBAQAAjAkIAABgTEAAAABjAgIAABgTEAAAwJiAAAAAxgQEAAAwJiAAAIAxAQEAAIwJCAAAYExAAAAAY9Xd654DAABwirADAQAAjAkIAABgTEAAAABjAgIAABgTEAAAwJiAAAAAxv4XMEfHjRvC/0EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method BatchNorm.call of <mrcnn.model.BatchNorm object at 0x0000023D7FE2CF60>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method BatchNorm.call of <mrcnn.model.BatchNorm object at 0x0000023D7FE2CF60>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method MaskRCNN.build.<locals>.ConstLayer.call of <mrcnn.model.MaskRCNN.build.<locals>.ConstLayer object at 0x0000023D02539BE0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method MaskRCNN.build.<locals>.ConstLayer.call of <mrcnn.model.MaskRCNN.build.<locals>.ConstLayer object at 0x0000023D02539BE0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method ProposalLayer.call of <mrcnn.model.ProposalLayer object at 0x0000023D02588C88>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method ProposalLayer.call of <mrcnn.model.ProposalLayer object at 0x0000023D02588C88>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method DetectionTargetLayer.call of <mrcnn.model.DetectionTargetLayer object at 0x0000023D025F1748>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method DetectionTargetLayer.call of <mrcnn.model.DetectionTargetLayer object at 0x0000023D025F1748>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method PyramidROIAlign.call of <mrcnn.model.PyramidROIAlign object at 0x0000023D025F1F98>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method PyramidROIAlign.call of <mrcnn.model.PyramidROIAlign object at 0x0000023D025F1F98>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: C:\\Projects\\Mask-RCNN-TF2.7.0-keras2.7.0\\logs\\shapes20220715T0955\\mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "rpn_model              (Functional)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n",
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/sub:0\", shape=(None,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/GatherV2_2:0\", shape=(None, 7, 7, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/Shape:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/sub_1:0\", shape=(None,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/GatherV2_5:0\", shape=(None, 7, 7, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/Shape_1:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/sub_2:0\", shape=(None,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/GatherV2_8:0\", shape=(None, 7, 7, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/Shape_2:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/sub_3:0\", shape=(None,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/GatherV2_11:0\", shape=(None, 7, 7, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/Shape_3:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/sub:0\", shape=(None,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/GatherV2_2:0\", shape=(None, 14, 14, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/Shape:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/sub_1:0\", shape=(None,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/GatherV2_5:0\", shape=(None, 14, 14, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/Shape_1:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/sub_2:0\", shape=(None,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/GatherV2_8:0\", shape=(None, 14, 14, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/Shape_2:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/sub_3:0\", shape=(None,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/GatherV2_11:0\", shape=(None, 14, 14, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/Shape_3:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_8_grad/Reshape_1:0\", shape=(4092,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_8_grad/Reshape:0\", shape=(4092, 4), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_8_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_9_grad/Reshape_1:0\", shape=(4092,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_9_grad/Reshape:0\", shape=(4092, 4), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_9_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_10_grad/Reshape_1:0\", shape=(4092,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_10_grad/Reshape:0\", shape=(4092, 4), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_10_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_11_grad/Reshape_1:0\", shape=(4092,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_11_grad/Reshape:0\", shape=(4092, 4), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_11_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_12_grad/Reshape_1:0\", shape=(4092,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_12_grad/Reshape:0\", shape=(4092, 4), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_12_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_13_grad/Reshape_1:0\", shape=(4092,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_13_grad/Reshape:0\", shape=(4092, 4), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_13_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_14_grad/Reshape_1:0\", shape=(4092,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_14_grad/Reshape:0\", shape=(4092, 4), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_14_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_15_grad/Reshape_1:0\", shape=(4092,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_15_grad/Reshape:0\", shape=(4092, 4), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_15_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - batch: 49.5000 - size: 8.0000 - loss: 6.6681 - rpn_class_loss: 0.1010 - rpn_bbox_loss: 2.8049 - mrcnn_class_loss: 0.8539 - mrcnn_bbox_loss: 2.2697 - mrcnn_mask_loss: 0.6386"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training_v1.py:2057: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "100/100 [==============================] - 57s 459ms/step - batch: 49.5000 - size: 8.0000 - loss: 6.6681 - rpn_class_loss: 0.1010 - rpn_bbox_loss: 2.8049 - mrcnn_class_loss: 0.8539 - mrcnn_bbox_loss: 2.2697 - mrcnn_mask_loss: 0.6386 - val_loss: 3.6265 - val_rpn_class_loss: 0.0414 - val_rpn_bbox_loss: 1.5357 - val_mrcnn_class_loss: 0.5179 - val_mrcnn_bbox_loss: 0.9706 - val_mrcnn_mask_loss: 0.5610\n"
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 1. LR=0.0001\n",
      "\n",
      "Checkpoint Path: C:\\Projects\\Mask-RCNN-TF2.7.0-keras2.7.0\\logs\\shapes20220715T0955\\mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "conv1                  (Conv2D)\n",
      "bn_conv1               (BatchNorm)\n",
      "res2a_branch2a         (Conv2D)\n",
      "bn2a_branch2a          (BatchNorm)\n",
      "res2a_branch2b         (Conv2D)\n",
      "bn2a_branch2b          (BatchNorm)\n",
      "res2a_branch2c         (Conv2D)\n",
      "res2a_branch1          (Conv2D)\n",
      "bn2a_branch2c          (BatchNorm)\n",
      "bn2a_branch1           (BatchNorm)\n",
      "res2b_branch2a         (Conv2D)\n",
      "bn2b_branch2a          (BatchNorm)\n",
      "res2b_branch2b         (Conv2D)\n",
      "bn2b_branch2b          (BatchNorm)\n",
      "res2b_branch2c         (Conv2D)\n",
      "bn2b_branch2c          (BatchNorm)\n",
      "res2c_branch2a         (Conv2D)\n",
      "bn2c_branch2a          (BatchNorm)\n",
      "res2c_branch2b         (Conv2D)\n",
      "bn2c_branch2b          (BatchNorm)\n",
      "res2c_branch2c         (Conv2D)\n",
      "bn2c_branch2c          (BatchNorm)\n",
      "res3a_branch2a         (Conv2D)\n",
      "bn3a_branch2a          (BatchNorm)\n",
      "res3a_branch2b         (Conv2D)\n",
      "bn3a_branch2b          (BatchNorm)\n",
      "res3a_branch2c         (Conv2D)\n",
      "res3a_branch1          (Conv2D)\n",
      "bn3a_branch2c          (BatchNorm)\n",
      "bn3a_branch1           (BatchNorm)\n",
      "res3b_branch2a         (Conv2D)\n",
      "bn3b_branch2a          (BatchNorm)\n",
      "res3b_branch2b         (Conv2D)\n",
      "bn3b_branch2b          (BatchNorm)\n",
      "res3b_branch2c         (Conv2D)\n",
      "bn3b_branch2c          (BatchNorm)\n",
      "res3c_branch2a         (Conv2D)\n",
      "bn3c_branch2a          (BatchNorm)\n",
      "res3c_branch2b         (Conv2D)\n",
      "bn3c_branch2b          (BatchNorm)\n",
      "res3c_branch2c         (Conv2D)\n",
      "bn3c_branch2c          (BatchNorm)\n",
      "res3d_branch2a         (Conv2D)\n",
      "bn3d_branch2a          (BatchNorm)\n",
      "res3d_branch2b         (Conv2D)\n",
      "bn3d_branch2b          (BatchNorm)\n",
      "res3d_branch2c         (Conv2D)\n",
      "bn3d_branch2c          (BatchNorm)\n",
      "res4a_branch2a         (Conv2D)\n",
      "bn4a_branch2a          (BatchNorm)\n",
      "res4a_branch2b         (Conv2D)\n",
      "bn4a_branch2b          (BatchNorm)\n",
      "res4a_branch2c         (Conv2D)\n",
      "res4a_branch1          (Conv2D)\n",
      "bn4a_branch2c          (BatchNorm)\n",
      "bn4a_branch1           (BatchNorm)\n",
      "res4b_branch2a         (Conv2D)\n",
      "bn4b_branch2a          (BatchNorm)\n",
      "res4b_branch2b         (Conv2D)\n",
      "bn4b_branch2b          (BatchNorm)\n",
      "res4b_branch2c         (Conv2D)\n",
      "bn4b_branch2c          (BatchNorm)\n",
      "res4c_branch2a         (Conv2D)\n",
      "bn4c_branch2a          (BatchNorm)\n",
      "res4c_branch2b         (Conv2D)\n",
      "bn4c_branch2b          (BatchNorm)\n",
      "res4c_branch2c         (Conv2D)\n",
      "bn4c_branch2c          (BatchNorm)\n",
      "res4d_branch2a         (Conv2D)\n",
      "bn4d_branch2a          (BatchNorm)\n",
      "res4d_branch2b         (Conv2D)\n",
      "bn4d_branch2b          (BatchNorm)\n",
      "res4d_branch2c         (Conv2D)\n",
      "bn4d_branch2c          (BatchNorm)\n",
      "res4e_branch2a         (Conv2D)\n",
      "bn4e_branch2a          (BatchNorm)\n",
      "res4e_branch2b         (Conv2D)\n",
      "bn4e_branch2b          (BatchNorm)\n",
      "res4e_branch2c         (Conv2D)\n",
      "bn4e_branch2c          (BatchNorm)\n",
      "res4f_branch2a         (Conv2D)\n",
      "bn4f_branch2a          (BatchNorm)\n",
      "res4f_branch2b         (Conv2D)\n",
      "bn4f_branch2b          (BatchNorm)\n",
      "res4f_branch2c         (Conv2D)\n",
      "bn4f_branch2c          (BatchNorm)\n",
      "res4g_branch2a         (Conv2D)\n",
      "bn4g_branch2a          (BatchNorm)\n",
      "res4g_branch2b         (Conv2D)\n",
      "bn4g_branch2b          (BatchNorm)\n",
      "res4g_branch2c         (Conv2D)\n",
      "bn4g_branch2c          (BatchNorm)\n",
      "res4h_branch2a         (Conv2D)\n",
      "bn4h_branch2a          (BatchNorm)\n",
      "res4h_branch2b         (Conv2D)\n",
      "bn4h_branch2b          (BatchNorm)\n",
      "res4h_branch2c         (Conv2D)\n",
      "bn4h_branch2c          (BatchNorm)\n",
      "res4i_branch2a         (Conv2D)\n",
      "bn4i_branch2a          (BatchNorm)\n",
      "res4i_branch2b         (Conv2D)\n",
      "bn4i_branch2b          (BatchNorm)\n",
      "res4i_branch2c         (Conv2D)\n",
      "bn4i_branch2c          (BatchNorm)\n",
      "res4j_branch2a         (Conv2D)\n",
      "bn4j_branch2a          (BatchNorm)\n",
      "res4j_branch2b         (Conv2D)\n",
      "bn4j_branch2b          (BatchNorm)\n",
      "res4j_branch2c         (Conv2D)\n",
      "bn4j_branch2c          (BatchNorm)\n",
      "res4k_branch2a         (Conv2D)\n",
      "bn4k_branch2a          (BatchNorm)\n",
      "res4k_branch2b         (Conv2D)\n",
      "bn4k_branch2b          (BatchNorm)\n",
      "res4k_branch2c         (Conv2D)\n",
      "bn4k_branch2c          (BatchNorm)\n",
      "res4l_branch2a         (Conv2D)\n",
      "bn4l_branch2a          (BatchNorm)\n",
      "res4l_branch2b         (Conv2D)\n",
      "bn4l_branch2b          (BatchNorm)\n",
      "res4l_branch2c         (Conv2D)\n",
      "bn4l_branch2c          (BatchNorm)\n",
      "res4m_branch2a         (Conv2D)\n",
      "bn4m_branch2a          (BatchNorm)\n",
      "res4m_branch2b         (Conv2D)\n",
      "bn4m_branch2b          (BatchNorm)\n",
      "res4m_branch2c         (Conv2D)\n",
      "bn4m_branch2c          (BatchNorm)\n",
      "res4n_branch2a         (Conv2D)\n",
      "bn4n_branch2a          (BatchNorm)\n",
      "res4n_branch2b         (Conv2D)\n",
      "bn4n_branch2b          (BatchNorm)\n",
      "res4n_branch2c         (Conv2D)\n",
      "bn4n_branch2c          (BatchNorm)\n",
      "res4o_branch2a         (Conv2D)\n",
      "bn4o_branch2a          (BatchNorm)\n",
      "res4o_branch2b         (Conv2D)\n",
      "bn4o_branch2b          (BatchNorm)\n",
      "res4o_branch2c         (Conv2D)\n",
      "bn4o_branch2c          (BatchNorm)\n",
      "res4p_branch2a         (Conv2D)\n",
      "bn4p_branch2a          (BatchNorm)\n",
      "res4p_branch2b         (Conv2D)\n",
      "bn4p_branch2b          (BatchNorm)\n",
      "res4p_branch2c         (Conv2D)\n",
      "bn4p_branch2c          (BatchNorm)\n",
      "res4q_branch2a         (Conv2D)\n",
      "bn4q_branch2a          (BatchNorm)\n",
      "res4q_branch2b         (Conv2D)\n",
      "bn4q_branch2b          (BatchNorm)\n",
      "res4q_branch2c         (Conv2D)\n",
      "bn4q_branch2c          (BatchNorm)\n",
      "res4r_branch2a         (Conv2D)\n",
      "bn4r_branch2a          (BatchNorm)\n",
      "res4r_branch2b         (Conv2D)\n",
      "bn4r_branch2b          (BatchNorm)\n",
      "res4r_branch2c         (Conv2D)\n",
      "bn4r_branch2c          (BatchNorm)\n",
      "res4s_branch2a         (Conv2D)\n",
      "bn4s_branch2a          (BatchNorm)\n",
      "res4s_branch2b         (Conv2D)\n",
      "bn4s_branch2b          (BatchNorm)\n",
      "res4s_branch2c         (Conv2D)\n",
      "bn4s_branch2c          (BatchNorm)\n",
      "res4t_branch2a         (Conv2D)\n",
      "bn4t_branch2a          (BatchNorm)\n",
      "res4t_branch2b         (Conv2D)\n",
      "bn4t_branch2b          (BatchNorm)\n",
      "res4t_branch2c         (Conv2D)\n",
      "bn4t_branch2c          (BatchNorm)\n",
      "res4u_branch2a         (Conv2D)\n",
      "bn4u_branch2a          (BatchNorm)\n",
      "res4u_branch2b         (Conv2D)\n",
      "bn4u_branch2b          (BatchNorm)\n",
      "res4u_branch2c         (Conv2D)\n",
      "bn4u_branch2c          (BatchNorm)\n",
      "res4v_branch2a         (Conv2D)\n",
      "bn4v_branch2a          (BatchNorm)\n",
      "res4v_branch2b         (Conv2D)\n",
      "bn4v_branch2b          (BatchNorm)\n",
      "res4v_branch2c         (Conv2D)\n",
      "bn4v_branch2c          (BatchNorm)\n",
      "res4w_branch2a         (Conv2D)\n",
      "bn4w_branch2a          (BatchNorm)\n",
      "res4w_branch2b         (Conv2D)\n",
      "bn4w_branch2b          (BatchNorm)\n",
      "res4w_branch2c         (Conv2D)\n",
      "bn4w_branch2c          (BatchNorm)\n",
      "res5a_branch2a         (Conv2D)\n",
      "bn5a_branch2a          (BatchNorm)\n",
      "res5a_branch2b         (Conv2D)\n",
      "bn5a_branch2b          (BatchNorm)\n",
      "res5a_branch2c         (Conv2D)\n",
      "res5a_branch1          (Conv2D)\n",
      "bn5a_branch2c          (BatchNorm)\n",
      "bn5a_branch1           (BatchNorm)\n",
      "res5b_branch2a         (Conv2D)\n",
      "bn5b_branch2a          (BatchNorm)\n",
      "res5b_branch2b         (Conv2D)\n",
      "bn5b_branch2b          (BatchNorm)\n",
      "res5b_branch2c         (Conv2D)\n",
      "bn5b_branch2c          (BatchNorm)\n",
      "res5c_branch2a         (Conv2D)\n",
      "bn5c_branch2a          (BatchNorm)\n",
      "res5c_branch2b         (Conv2D)\n",
      "bn5c_branch2b          (BatchNorm)\n",
      "res5c_branch2c         (Conv2D)\n",
      "bn5c_branch2c          (BatchNorm)\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "rpn_model              (Functional)\n",
      "anchors                (ConstLayer)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training_2/SGD/gradients/gradients/roi_align_classifier/concat_grad/sub:0\", shape=(None,), dtype=int32), values=Tensor(\"training_2/SGD/gradients/gradients/roi_align_classifier/concat_grad/GatherV2_2:0\", shape=(None, 7, 7, 256), dtype=float32), dense_shape=Tensor(\"training_2/SGD/gradients/gradients/roi_align_classifier/concat_grad/Shape:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training_2/SGD/gradients/gradients/roi_align_classifier/concat_grad/sub_1:0\", shape=(None,), dtype=int32), values=Tensor(\"training_2/SGD/gradients/gradients/roi_align_classifier/concat_grad/GatherV2_5:0\", shape=(None, 7, 7, 256), dtype=float32), dense_shape=Tensor(\"training_2/SGD/gradients/gradients/roi_align_classifier/concat_grad/Shape_1:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training_2/SGD/gradients/gradients/roi_align_classifier/concat_grad/sub_2:0\", shape=(None,), dtype=int32), values=Tensor(\"training_2/SGD/gradients/gradients/roi_align_classifier/concat_grad/GatherV2_8:0\", shape=(None, 7, 7, 256), dtype=float32), dense_shape=Tensor(\"training_2/SGD/gradients/gradients/roi_align_classifier/concat_grad/Shape_2:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training_2/SGD/gradients/gradients/roi_align_classifier/concat_grad/sub_3:0\", shape=(None,), dtype=int32), values=Tensor(\"training_2/SGD/gradients/gradients/roi_align_classifier/concat_grad/GatherV2_11:0\", shape=(None, 7, 7, 256), dtype=float32), dense_shape=Tensor(\"training_2/SGD/gradients/gradients/roi_align_classifier/concat_grad/Shape_3:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training_2/SGD/gradients/gradients/roi_align_mask/concat_grad/sub:0\", shape=(None,), dtype=int32), values=Tensor(\"training_2/SGD/gradients/gradients/roi_align_mask/concat_grad/GatherV2_2:0\", shape=(None, 14, 14, 256), dtype=float32), dense_shape=Tensor(\"training_2/SGD/gradients/gradients/roi_align_mask/concat_grad/Shape:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training_2/SGD/gradients/gradients/roi_align_mask/concat_grad/sub_1:0\", shape=(None,), dtype=int32), values=Tensor(\"training_2/SGD/gradients/gradients/roi_align_mask/concat_grad/GatherV2_5:0\", shape=(None, 14, 14, 256), dtype=float32), dense_shape=Tensor(\"training_2/SGD/gradients/gradients/roi_align_mask/concat_grad/Shape_1:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training_2/SGD/gradients/gradients/roi_align_mask/concat_grad/sub_2:0\", shape=(None,), dtype=int32), values=Tensor(\"training_2/SGD/gradients/gradients/roi_align_mask/concat_grad/GatherV2_8:0\", shape=(None, 14, 14, 256), dtype=float32), dense_shape=Tensor(\"training_2/SGD/gradients/gradients/roi_align_mask/concat_grad/Shape_2:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training_2/SGD/gradients/gradients/roi_align_mask/concat_grad/sub_3:0\", shape=(None,), dtype=int32), values=Tensor(\"training_2/SGD/gradients/gradients/roi_align_mask/concat_grad/GatherV2_11:0\", shape=(None, 14, 14, 256), dtype=float32), dense_shape=Tensor(\"training_2/SGD/gradients/gradients/roi_align_mask/concat_grad/Shape_3:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training_2/SGD/gradients/gradients/ROI/GatherV2_8_grad/Reshape_1:0\", shape=(4092,), dtype=int32), values=Tensor(\"training_2/SGD/gradients/gradients/ROI/GatherV2_8_grad/Reshape:0\", shape=(4092, 4), dtype=float32), dense_shape=Tensor(\"training_2/SGD/gradients/gradients/ROI/GatherV2_8_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training_2/SGD/gradients/gradients/ROI/GatherV2_9_grad/Reshape_1:0\", shape=(4092,), dtype=int32), values=Tensor(\"training_2/SGD/gradients/gradients/ROI/GatherV2_9_grad/Reshape:0\", shape=(4092, 4), dtype=float32), dense_shape=Tensor(\"training_2/SGD/gradients/gradients/ROI/GatherV2_9_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training_2/SGD/gradients/gradients/ROI/GatherV2_10_grad/Reshape_1:0\", shape=(4092,), dtype=int32), values=Tensor(\"training_2/SGD/gradients/gradients/ROI/GatherV2_10_grad/Reshape:0\", shape=(4092, 4), dtype=float32), dense_shape=Tensor(\"training_2/SGD/gradients/gradients/ROI/GatherV2_10_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training_2/SGD/gradients/gradients/ROI/GatherV2_11_grad/Reshape_1:0\", shape=(4092,), dtype=int32), values=Tensor(\"training_2/SGD/gradients/gradients/ROI/GatherV2_11_grad/Reshape:0\", shape=(4092, 4), dtype=float32), dense_shape=Tensor(\"training_2/SGD/gradients/gradients/ROI/GatherV2_11_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training_2/SGD/gradients/gradients/ROI/GatherV2_12_grad/Reshape_1:0\", shape=(4092,), dtype=int32), values=Tensor(\"training_2/SGD/gradients/gradients/ROI/GatherV2_12_grad/Reshape:0\", shape=(4092, 4), dtype=float32), dense_shape=Tensor(\"training_2/SGD/gradients/gradients/ROI/GatherV2_12_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training_2/SGD/gradients/gradients/ROI/GatherV2_13_grad/Reshape_1:0\", shape=(4092,), dtype=int32), values=Tensor(\"training_2/SGD/gradients/gradients/ROI/GatherV2_13_grad/Reshape:0\", shape=(4092, 4), dtype=float32), dense_shape=Tensor(\"training_2/SGD/gradients/gradients/ROI/GatherV2_13_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training_2/SGD/gradients/gradients/ROI/GatherV2_14_grad/Reshape_1:0\", shape=(4092,), dtype=int32), values=Tensor(\"training_2/SGD/gradients/gradients/ROI/GatherV2_14_grad/Reshape:0\", shape=(4092, 4), dtype=float32), dense_shape=Tensor(\"training_2/SGD/gradients/gradients/ROI/GatherV2_14_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training_2/SGD/gradients/gradients/ROI/GatherV2_15_grad/Reshape_1:0\", shape=(4092,), dtype=int32), values=Tensor(\"training_2/SGD/gradients/gradients/ROI/GatherV2_15_grad/Reshape:0\", shape=(4092, 4), dtype=float32), dense_shape=Tensor(\"training_2/SGD/gradients/gradients/ROI/GatherV2_15_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - batch: 49.5000 - size: 8.0000 - loss: 4.8878 - rpn_class_loss: 0.0241 - rpn_bbox_loss: 0.7079 - mrcnn_class_loss: 0.5161 - mrcnn_bbox_loss: 0.6599 - mrcnn_mask_loss: 0.5359"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training_v1.py:2057: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "100/100 [==============================] - 68s 538ms/step - batch: 49.5000 - size: 8.0000 - loss: 4.8878 - rpn_class_loss: 0.0241 - rpn_bbox_loss: 0.7079 - mrcnn_class_loss: 0.5161 - mrcnn_bbox_loss: 0.6599 - mrcnn_mask_loss: 0.5359 - val_loss: 4.3590 - val_rpn_class_loss: 0.0193 - val_rpn_bbox_loss: 0.5986 - val_mrcnn_class_loss: 0.4572 - val_mrcnn_bbox_loss: 0.5681 - val_mrcnn_mask_loss: 0.5362\n"
     ]
    }
   ],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=2, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method DetectionLayer.call of <mrcnn.model.DetectionLayer object at 0x00000240C80667B8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method DetectionLayer.call of <mrcnn.model.DetectionLayer object at 0x00000240C80667B8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:From c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:620: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "WARNING:tensorflow:AutoGraph could not transform <function refine_detections_graph.<locals>.nms_keep_map at 0x0000023D02554268> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function refine_detections_graph.<locals>.nms_keep_map at 0x0000023D02554268> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Loading weights from  C:\\Projects\\Mask-RCNN-TF2.7.0-keras2.7.0\\logs\\shapes20220715T0955\\mask_rcnn_shapes_0002.h5\n",
      "Re-starting from epoch 2\n"
     ]
    }
   ],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_image           shape: (128, 128, 3)         min:   98.00000  max:  177.00000  uint8\n",
      "image_meta               shape: (16,)                 min:    0.00000  max:  128.00000  int32\n",
      "gt_class_id              shape: (1,)                  min:    1.00000  max:    1.00000  int32\n",
      "gt_bbox                  shape: (1, 4)                min:   49.00000  max:  101.00000  int32\n",
      "gt_mask                  shape: (128, 128, 1)         min:    0.00000  max:    1.00000  bool\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 images\n",
      "image                    shape: (128, 128, 3)         min:   98.00000  max:  177.00000  uint8\n",
      "molded_images            shape: (1, 128, 128, 3)      min:   -5.90000  max:   62.10000  float64\n",
      "image_metas              shape: (1, 16)               min:    0.00000  max:  128.00000  int32\n",
      "anchors                  shape: (1, 4092, 4)          min:   -0.71267  max:    1.20874  float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** No instances to display *** \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1152 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
